---
title: "BiciMad. Electric Bikes in Madrid"
author: "Bruno Sanz Marino"
date: "12-31-2019"
output: pdf_document
---
Table of Contents
---------------------------------------------------
1. [BICIMAD Goals and Object of Study](#Goals)

1.1. [About Madrid](#AboutMadrid)
  
1.2. [What is BICIMAD](#WhatIsBICIMAD)
  
2. [Dataset Build](#DatasetBuild)

2.1. [Data Recolection](#DataRecolection)

2.2. [Data Cleaning](#DataCleaning)

3. [EDA. Exploratory Data Analysis](#EDA)

4. [Models Construction](#Models)

4.1. [Selection Metodology](#SelectionMetodology)

4.2. [Linear Regression](#LinearRegression)

4.3. [Non Linear Regression](#NonLinearRegression)

4.4. [Shrinkage Regression](#Shrinkage)

4.5. [Ensemble Methods.Trees](#EnsembleMethods)

5. [Conclussions](#Conclusions)

6. [Future Investigations](#FutureInvestigations)

7. [Bibliography and Web References](#Bibliography)

```{r IMPORT setup, include=FALSE, warning=FALSE, error=FALSE}
# IMPORT OF THE LIBRARIES WE NEED TO EXECUTE THE CODE
knitr::opts_chunk$set(echo = TRUE)
library(rjson)
library(tidyverse)
library(lubridate)
library(purrr)
library(corrplot)
library(mgcv)
library(PerformanceAnalytics)
library(glmnet)
library(caret)
library(ggrepel)
library(randomForest)
library(car)
library(gvlma)
library(readr)
library(gbm)
```
```{r DOWNLOAD FILES, echo=FALSE, warning=FALSE, results = 'hide'}
#DOWNLOAD THE FILES FOR THE CODE
#setwd("//Users/brunosanzmarino/OneDrive/formacion/data_Science_capstone/bicimad/bicis_usos_acumulado")
#READ FILES
getwd()
tabla_usos <- read_csv2(tabla_usos_url)
view(tabla_usos)
tabla_usos <- read_csv2("tabla_usos.csv",col_names=TRUE)
tabla_usos_festivos <- read_csv2("tabla_dias_festivos.csv",col_names=TRUE)
#view(tabla_usos_festivos)
tabla_disponibilidad <- read_csv2("bici_disponibilidad.csv",col_names=TRUE)
#setwd("//Users/brunosanzmarino/OneDrive/formacion/data_Science_capstone/bicimad/bicis_usuarios_abonos")
tabla_abonos <- read_csv2("abono_anual_activos.csv",col_names=TRUE)
setwd("//Users/brunosanzmarino/OneDrive/formacion/data_Science_capstone/clima")
AEMET_DATAA <- fromJSON(file="2014_2017.txt")
AEMET_DATAB <- fromJSON(file="2018_2019.txt")

```


```{r DATASET_FUNCTIONS, echo=FALSE, warning=FALSE, results = 'hide'}
set.seed(3456)

#GRID VALUES FOR LASSORIDGE
grid <-10^seq(10,-2,length=100)

#TYPES OF REGULARIZATION MODEL
LASSOGLMNET = 1
RIDGEGLMNET = 0
ELASTICGLMNET = 0.5

#PARSE LATIN NUMBERS
spanish_locale <- locale(grouping_mark=".", decimal_mark=",")

# RMSE RESULTS
RMSELINEAL <- 0
RMSEGAM <- 0
RMSERIDGEFIT <- 0
RMSELASSOFIT <- 0
RMSEELASTICFIT <- 0
RMSERANDOMFOREST <- 0
RMSEGBM <- 0

#SCALE VARIABLES FOR MODELING
scaleVariables <- function()
{
  tabla_all <- tabla_all  %>% mutate(AEMET_RAINFALLS_SCALE = scale(AEMET_RAINFALLS))
  tabla_all <- tabla_all  %>% mutate(AEMET_TA_SCALE = scale(AEMET_TA))
  tabla_all <- tabla_all  %>% mutate(AEMET_TM_SCALE = scale(AEMET_TM))
  #tabla_all <- tabla_all  %>% mutate(RUAAADI_SCALE = scale(PAYMENT_UAAADI))
  tabla_all <- tabla_all  %>% mutate(PAYMENT_UAAASTA_SCALE = scale(PAYMENT_UAAASTA))
  tabla_all <- tabla_all  %>% mutate(PAYMENT_UAAACTA_SCALE = scale(PAYMENT_UAAACTA))
  tabla_all <- tabla_all  %>% mutate(PAYMENT_UAAADI_SCALE = scale(PAYMENT_UAAADI))
 
  tabla_all <- tabla_all %>% mutate(USE_BIKE_MBD_SCALE = scale(USE_BIKE_MBD))
  
   tabla_all <- tabla_all %>% mutate(AEMET_HORATMIN_SCALE = scale(AEMET_HORATMIN))
   tabla_all <- tabla_all %>% mutate(AEMET_VELMEDIA_SCALE = scale(AEMET_VELMEDIA))
   tabla_all <- tabla_all %>% mutate(AEMET_RACHA_SCALE = scale(AEMET_RACHA))
   tabla_all <- tabla_all %>% mutate(AEMET_PRESMAX_SCALE = scale(AEMET_PRESMAX))
   tabla_all <- tabla_all %>% mutate(AEMET_TMIN_SCALE = scale(AEMET_TMIN))
   tabla_all <- tabla_all %>% mutate(AEMET_TMAX_SCALE = scale(AEMET_TMAX))
  

  
  result <- tabla_all
  return(tabla_all)
}
#RMSE METHOD FOR MODEL VALIDATION
RMSE <- function(true_ratings, predicted_ratings)
{
  sqrt(mean((true_ratings - predicted_ratings)^2))

}
#LITERALS OF MONTHS
LiteralMonth <- function(num)
{
  result <- "";
  if(num == 1)
  { result <- "Jan"}
  if(num == 2)
  { result <- "Feb"}
  if(num == 3)
  { result <- "March"}
  if(num == 4)
  { result <- "April"}
  if(num == 5)
  { result <- "May"}
  if(num == 6)
  { result <- "June"}
  if(num == 7)
  { result <- "July"}
  if(num == 8)
  { result <- "Aug"}
  if(num == 9)
  { result <- "Sep"}
  if(num == 10)
  { result <- "Oct"}
  if(num == 11)
  { result <- "Nov"}
  if(num == 12)
  { result <- "Dec"}
  return(result)
  
}
#DUMMY DAY OF WEEK VARIABLES
EnglishDayOfWeek <- function (literal)
{
  result <- ""
  
       if (literal =="Lun") 
         {result <- "Mon"}
       if (literal =="Mar") 
         {result<- "Tue"}
       if (literal =="Mié" ) 
         {result<-  "Wed"}
       if (literal =="Jue" ) 
         {result<- "Thru"}
       if (literal =="Vie") 
         {result<- "Fri"}
       if (literal =="Sáb") 
         {result<- "Sat"}
       if (literal =="Dom") 
         {result<- "Sun"}
  return(result)
  
}
TypeOfDay <- function (literal)
{
  result <- ""
       if (literal ==0) 
         {result <- "Working Day"}
       if (literal ==1) 
       {result<- "Holiday"}
  return(result)

  }
NumDayOfWeek <- function(literal)
{
  result <- 0
  
       if (literal =="Lun") 
         {result <- 1}
       if (literal =="Mar") 
         {result<- 2}
       if (literal =="Mié" ) 
         {result<-  3}
       if (literal =="Jue" ) 
         {result<- 4}
       if (literal =="Vie") 
         {result<- 5}
       if (literal =="Sáb") 
         {result<- 6}
       if (literal =="Dom") 
         {result<- 7}
  return(result)
  
  
}

GenerateTimeFields <- function()
{
  tabla_all <- tabla_all %>% rename(DAY_OF_WEEK = `Día semana`)
  tabla_all <- tabla_all %>% mutate(DAY_OF_WEEK_INT = sapply(tabla_all$DAY_OF_WEEK,NumDayOfWeek))
  tabla_all <- tabla_all %>% mutate(DAY_OF_WEEK_EN = sapply(tabla_all$DAY_OF_WEEK,EnglishDayOfWeek))
  tabla_all <- tabla_all %>% mutate(HOLIDAY_EN = sapply(tabla_all$HOLIDAY,TypeOfDay))

    

                            
      
  
  tabla_all <- tabla_all %>%
    mutate(
      MONDAY = ifelse(DAY_OF_WEEK == "Lun", 1, 0),
      TUESDAY = ifelse(DAY_OF_WEEK == "Mar", 1, 0),
      WEDNESDAY = ifelse(DAY_OF_WEEK == "Mié", 1, 0),
      THRUSDAY = ifelse(DAY_OF_WEEK == "Jue", 1, 0),
      FRIDAY = ifelse(DAY_OF_WEEK == "Vie", 1, 0),
      SATURDAY = ifelse(DAY_OF_WEEK == "Sáb", 1, 0),
      SUNDAY = ifelse(DAY_OF_WEEK == "Dom", 1, 0)
    )
  result <- tabla_all
  return(tabla_all)
}
#TEMPERATURE VARIABLE MODIFICATION
MedianTemp <- function()
{
  AGRUPADO <- tabla_all %>% group_by(AEMET_TM)  %>% summarise(median(USE_BIKE_TOTAL_USE),n())
#  view(AGRUPADO)
  MAX_AGRUPADO <- max(AGRUPADO$`median(USE_BIKE_TOTAL_USE)`)
  MAX_ROW <- filter(AGRUPADO,`median(USE_BIKE_TOTAL_USE)` == MAX_AGRUPADO)
  result <- MAX_ROW$AEMET_TM
    return(result)
}
#MISSING VALUES
Solve_MA <- function()
{
  
  TMMean <- round(mean(tabla_all$AEMET_TM,na.rm=TRUE))
  tabla_all$AEMET_TM <- replace_na(tabla_all$AEMET_TM,TMMean)
  TAMean <- round(mean(tabla_all$AEMET_TA,na.rm=TRUE))
  tabla_all$AEMET_TA <- replace_na(tabla_all$AEMET_TA,TAMean)
  reainfallsMean <- mean(tabla_all$AEMET_RAINFALLS,na.rm=TRUE)
  tabla_all$AEMET_RAINFALLS <- replace_na(tabla_all$AEMET_RAINFALLS,reainfallsMean)
  TMINMEAN <- mean(tabla_all$AEMET_TMIN,na.rm=TRUE)
  tabla_all$AEMET_TMIN <- replace_na(tabla_all$AEMET_TMIN,TMINMEAN)
  HORATMINMEAN <- mean(tabla_all$AEMET_HORATMIN,na.rm=TRUE)
  tabla_all$AEMET_HORATMIN<- replace_na(tabla_all$AEMET_HORATMIN, HORATMINMEAN)
  VELMEDIA <- mean(tabla_all$AEMET_VELMEDIA,na.rm=TRUE)
  tabla_all$AEMET_VELMEDIA<- replace_na(tabla_all$AEMET_VELMEDIA,VELMEDIA)
  RACHAMEDIA <-mean(tabla_all$AEMET_RACHA,na.rm=TRUE)
  tabla_all$AEMET_RACHA <- replace_na(tabla_all$AEMET_RACHA, RACHAMEDIA)
  TMAXMEDIA <--mean(tabla_all$AEMET_TMAX,na.rm=TRUE)
  tabla_all$AEMET_TMAX <- replace_na(tabla_all$AEMET_TMAX,TMAXMEDIA)
  PRESMAXMEDIA <-mean(tabla_all$AEMET_PRESMAX,na.rm=TRUE)
  tabla_all$AEMET_PRESMAX<- replace_na(tabla_all$AEMET_PRESMAX,PRESMAXMEDIA)
  
  result <- tabla_all;
  return(tabla_all)
  
}
#WEATHER VARIABLES PARSING TO NUMBERS
mapDateFieldsAEMET <- function ()
{
  AEMETTIDY <- AEMETTIDY  %>% mutate(DAY = as.Date(ymd(fecha)))
  AEMETTIDY <-  AEMETTIDY %>% mutate(YEAR = year(DAY))
  AEMETTIDY <-  AEMETTIDY %>% mutate(MONTH = month(DAY))
  AEMETTIDY <- AEMETTIDY %>% mutate(LABEL_MONTH = sapply(AEMETTIDY$MONTH,LiteralMonth))

  AEMETTIDY <-  AEMETTIDY %>% mutate(AEMET_RAINFALLS = as.numeric(sub(",", ".", prec, fixed = TRUE)))
  AEMETTIDY <-  AEMETTIDY %>% mutate(AEMET_RAINFALLS_INT = round(AEMET_RAINFALLS))
  AEMETTIDY <-  AEMETTIDY %>% mutate(AEMET_TM = as.numeric(sub(",", ".", tmed, fixed = TRUE)))
  AEMETTIDY <- AEMETTIDY %>% mutate(AEMET_TMED = parse_number(tmed,locale=spanish_locale))
  AEMETTIDY <- AEMETTIDY %>% mutate(AEMET_PREC = parse_number(prec,locale=spanish_locale))
  AEMETTIDY <- AEMETTIDY %>% mutate(AEMET_TMIN = parse_number(tmin,locale=spanish_locale))
  AEMETTIDY <- AEMETTIDY %>% mutate(AEMET_HORATMIN = parse_number(horatmin,locale=spanish_locale))
  AEMETTIDY <- AEMETTIDY %>% mutate(AEMET_VELMEDIA = parse_number(velmedia,locale=spanish_locale))
  AEMETTIDY <- AEMETTIDY %>% mutate(AEMET_RACHA = parse_number(racha,locale=spanish_locale))
  AEMETTIDY <- AEMETTIDY %>% mutate(AEMET_TMAX = parse_number(tmax,locale=spanish_locale))
  AEMETTIDY <- AEMETTIDY %>% mutate(AEMET_PRESMAX = parse_number(presmax,locale=spanish_locale))
  result <- AEMETTIDY
  
  return(AEMETTIDY)
  
}
#MAP FROM JSON STRUCTURE TO TIDY
mapAEMET <- function(DATA,TIDY)
{
  aemetLength <- length(DATA)
  for (n in 1:aemetLength)
  {
    f <- DATA[n][[1]]
    try(TIDY$fecha[n] <- f$fecha)
    try(TIDY$indicativo[n] <-f$indicativo)
    try(TIDY$nombre[n] <-f$nombre)
    try(TIDY$provincia[n] <-f$provincia)
    try(TIDY$altitud[n] <-f$altitud)
    try(TIDY$tmed[n] <-f$tmed)
    try(TIDY$prec[n] <- f$prec)
    try(TIDY$tmin[n] <-f$tmin)
    try(TIDY$horatmin[n] <-f$horatmin)
    try(TIDY$tmax[n]<-f$tmax)
    try(TIDY$horatmax[n] <- f$horatmax)
    try(TIDY$velmedia[n] <- f$velmedia)
    try(TIDY$racha[n] <- f$racha)
    try(TIDY$horaracha[n] <- f$horaracha)
    try(TIDY$presmax[n] <- f$presMax)
    try(TIDY$horapresmax[n] <- f$horaPresMax)
    try(TIDY$presmin[n] <- f$presMin)
    try(TIDY$horapresmin[n] <-f$horaPresMin)
  }
  result <- TIDY
  return(TIDY)


}
# CREATE AEMET WEATHER DATASET
cleanAEMET <- function(AEMET_DATAA,AEMET_DATAB)
{
  aemetLengthA <- length(AEMET_DATAA)
  aemetLengthB <- length(AEMET_DATAB)
  tbl_colnames <- c("fecha","indicativo"  ,"nombre" ,"provincia","altitud","tmed" ,"prec" ,"tmin" ,"horatmin" ,"tmax" ,"horatmax" ,"dir" ,"velmedia" ,"racha" ,"horaracha","presmax","horapresmax"  ,"presmin","horapresmin","dia")
  AEMETTIDY1 <- as_tibble(data.frame(matrix(nrow=aemetLengthA,ncol=length(tbl_colnames))))
  AEMETTIDY2 <- as_tibble(data.frame(matrix(nrow=aemetLengthB,ncol=length(tbl_colnames))))
  colnames(AEMETTIDY1) <- tbl_colnames
  colnames(AEMETTIDY2) <- tbl_colnames
  AEMETTIDY1 <- mapAEMET(AEMET_DATAA,AEMETTIDY1)
  AEMETTIDY2 <- mapAEMET(AEMET_DATAB,AEMETTIDY2)
  result <- rbind(AEMETTIDY1,AEMETTIDY2)
  result <- as_tibble(result)
  return(result)
}
#RENAME BICIMAD COLUMNS
cleanBICIMADAbonos <- function(tabla_abonos)
{
  
  dropUsos.cols <- c('X5','X9','X10','X11','X12','X13','X14','X15','X16','X17','Día','Altas nuevos usuarios activos ab. anual sin ttp día','Altas nuevos usuarios activos ab. anual con ttp día','Altas nuevos usuarios activos ab. anual total día')
  #,'Altas nuevos usuarios activos ab. anual sin ttp día','Altas nuevos usuarios activos ab. anual con ttp #día','Altas nuevos usuarios activos ab. anual total día'
  #view(tabla_abonos)
  tabla_abonos <- tabla_abonos  %>% mutate(DAY = as.Date(mdy(Día)))
  
  tabla_abonos <- tabla_abonos %>% select(-dropUsos.cols)
  #tabla_abonos <- tabla_abonos %>% rename( "ALTAS_USU_ANU_SIN_TTP_DIA"="Altas nuevos usuarios activos ab. anual sin ttp día")
  #tabla_abonos <- tabla_abonos %>% rename( "ALTAS_USU_ANU_CON_TTP_DIA"="Altas nuevos usuarios activos ab. anual con ttp día")
 # tabla_abonos <- tabla_abonos %>% rename( "ALTAS_USU_ABO_ANUAL_TOTAL_DIA"="Altas nuevos usuarios activos ab. anual total día")
  #USUARIOS_ACTIVOS_ABO_ANUAL_SIN_TTP_ACUMULADO
  tabla_abonos <- tabla_abonos %>% rename( "PAYMENT_UAAASTA"="Usuarios activos ab. anual sin ttp acumulado desde inicio")
  #USUARIOS_ACTIVOS_ABO_ANUAL_CON_TTP_ACUMULADO
  tabla_abonos <- tabla_abonos %>% rename( "PAYMENT_UAAACTA"="Usuarios activos ab. anual con ttp acumulado desde inicio")
  #USUARIOS_ACTIVOS_ABO_ANUAL_ACUMULADO_DESDE_INICIO
  tabla_abonos <- tabla_abonos %>% rename( "PAYMENT_UAAADI"="Usuarios activos ab. anual total acumulado desde inicio")
 
  result <- tabla_abonos
  return(tabla_abonos)
}
#RENAME AND CLEAN BICIMAD
cleanBICIMADUsos <- function(tabla_usos)
{
  #DELETED COLUMNS
  dropUsos.cols <- c('X5','X9','X11','X12','X13','X14','X15','X16','X17','X18','X19')
 
  #)
  #,'fecha','ACUMULADO_MES_TOTAL')
  dropFestivos.cols <- c('X4','X5')
  #CAST TO tibble
  tabla_usos <- as_tibble(tabla_usos)
  tabla_usos_festivos <- as_tibble(tabla_usos_festivos)

  #RENAME FIELDS
  #USOS_ANUAL
  tabla_usos <- tabla_usos %>% rename( "USE_BIKE_UA"="Usos bicis abono anual")
  #USOS_OCASIONAL
  tabla_usos <- tabla_usos %>% rename( "USE_BIKE_UO"="Usos bicis abono ocasional")
  #ACUMULADO_MES_TOTAL
  tabla_usos <- tabla_usos %>% rename( "USE_BIKE_TOTAL_USE"="Usos bicis total")
  #ACUMULADO_MES_TOTAL
  tabla_usos <- tabla_usos %>% rename( "USE_BIKE_AMT"="Acumulado mes total")
  #USOS_ABONO_ANUAL_ACUMULADO
  tabla_usos <- tabla_usos %>% rename( "USE_BIKE_UAAA"="Usos abono anual acumulado")
  #USOS_ABONO_OCASIONAL_ACUMULADO
  tabla_usos <- tabla_usos %>% rename( "USE_BIKE_UAOA"="Usos abono ocasional acumulado")
  #USOS_TOTAL_ACUMULADO_DESDE_INICIO
  tabla_usos <- tabla_usos %>% rename( "USE_BIKE_UTADI"="Usos total acumulado desde inicio")
  
  tabla_usos <- tabla_usos %>% select(-dropUsos.cols)
  tabla_usos_festivos <- tabla_usos_festivos %>% select(-dropFestivos.cols)
  #view(tabla_usos)
  tabla_usos <- tabla_usos  %>% mutate(DAY = as.Date(mdy(DIA)))
  tabla_disponibilidad <- tabla_disponibilidad  %>% mutate(DAY = as.Date(dmy(DIA)))
  tabla_usos_festivos <- tabla_usos_festivos %>% mutate(DAY = as.Date(mdy(Fecha)))
  tabla_usos_festivos$`Sab, dom, festivos` <-  tabla_usos_festivos$`Sab, dom, festivos` %>% replace_na("NA")
  tabla_usos_festivos <- tabla_usos_festivos %>% mutate(HOLIDAY= if_else(tabla_usos_festivos$"Sab, dom, festivos"== "NA", 0, 1))
  # JOIN TABLES INTO ONE
  tabla_usos <- inner_join(tabla_usos,tabla_usos_festivos, by="DAY")
  tabla_usos <- inner_join(tabla_usos,tabla_disponibilidad, by="DAY")
  tabla_usos <- inner_join(tabla_usos,tabla_abonos, by="DAY")
  
 
  dropUsosColumns <- c('DIA.x','DIA.y','Sab, dom, festivos','HORAS_TOTALES_USOS_BICICLETAS','HORAS_TOTALES_DISPONIBILIDAD_BICICLETAS_EN_ANCLAJES','TOTAL_HORAS_SERVICIO_BICICLETAS','TOTAL_USOS','FECHA')
  #tabla_usos <- tabla_usos %>% select(-dropUsosColumns)
  result <- tabla_usos
  #view(tabla_usos)
  return(tabla_usos)

}

```
```{r BICIMAD_DATASET_CREATION, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE,results = 'hide'}

```


```{r BICIMAD_DATASET_CREATION, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE,results = 'hide'}
tabla_disponibilidad <- tabla_disponibilidad %>% rename( "USE_BIKE_MBD"="MEDIA_BICICLETAS_DISPONIBLES")

view(tabla_disponibilidad)
#TIBBLE
tabla_abonos <- cleanBICIMADAbonos(tabla_abonos)
#view(tabla_abonos)

tabla_usos <- cleanBICIMADUsos(tabla_usos)
tabla_usos_drop <- c("DIA.x","USE_BIKE_AMT","Sab, dom, festivos","DIA.y","HORAS_TOTALES_USOS_BICICLETAS","HORAS_TOTALES_DISPONIBILIDAD_BICICLETAS_EN _ANCLAJES","TOTAL_HORAS_SERVICIO_BICICLETAS")
tabla_usos <- select(tabla_usos,-tabla_usos_drop)
```

```{r AEMET_DATA_SET_CREATION,echo=FALSE, warning=FALSE, error=FALSE, message=FALSE,results = 'hide'}

AEMETTIDY <-cleanAEMET(AEMET_DATAA,AEMET_DATAB)
AEMETTIDY <- mapDateFieldsAEMET()
dropAemetCols <- c('indicativo','nombre','provincia','altitud','dir','velmedia','racha','horaracha','presmax','horapresmax','presmin','horapresmin','dia');
AEMETTIDY <- select(AEMETTIDY,-dropAemetCols)


AEMETTIDY <- AEMETTIDY  %>% mutate(AEMET_TM = trunc(AEMET_TM))
```

```{r DATASET_JOIN, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE,results = 'hide'}

tabla_all <- inner_join(tabla_usos,AEMETTIDY,by="DAY")
tabla_all <- GenerateTimeFields()

mt <- MedianTemp()

tabla_all <- tabla_all %>% mutate(AEMET_TA =-abs(mt - abs(AEMET_TM)))


tabla_all <- Solve_MA()

tabla_all <- scaleVariables()
```


1 BICIMAD Goals and Object of Study<a name="Goals"></a>
---------------------------------------------------

The main goal of this analysis is the study of the use that Madrid people (Spain) make of BICIMAD. 

BICIMAD is the the public electric bike rental service that the city hall offers to the citizens and visitors.

## 1.1 About Madrid<a name="AboutMadrid"></a>

## 1.2 ¿Que es BICIMAP?<a name="WhatIsBICIMAD"></a>

The service works through payment subscriptions. They can be annual or casual (one day, three days or five days) which allow users to take bikes from any part of the city (not the suburbs), and use them for a fixed amount of time.

The City Council has created a series of anchors to store the bikes, when they are not in use.

BICIMAD has been available since 2014, and today (end of 2019), offers more than 2000 bikes and 165 parking stations.

The user can know, in real time, the availability of bikes in each station, as well as the number of free anchors.

2. Dataset Build<a name="DatasetBuild"></a>
---------------------------------------------------

## 2.1 Data Recolection<a name="DataRecolection"></a>

In order to be able to analyze the public service, BICIMAD, i am going to build one dataset from two main data sources. 

I am going to use the data provided by the EMT (Municipal transport company of Madrid) about uses and the use cards. These datasets are available on the public open data portal from the Town Hall (datos.madrid.es). This portal, offer many kind of datasets from the different areas of activity of the city.

The info that we are going to collect from this study is diary.

In addition, we are going to use weather information, that comes from AEMET (aemet.es). AEMET is the National Meteorological Agency of Spain. We are going to extract info about rainfalls, temperature and other weather measures.

AEMET offer info about each one of the weather stations of the country. We are going to choose just one, located inside the city. The one named "Retiro Station". 

Once we have this two datasets, we are going to join them to build the main dataset for the analysis. The link field will be te date, since both datasets comes in a daily basis.


A continuación detallamos los campos extraidos que formarán parte de nuestro dataset.

```{r echo=FALSE, warning=FALSE, error=FALSE}

DS_FIELDS <- tibble(COLUMN="USE_BIKE_UA",ORIGIN="BICIMAD",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="USE_BIKE_UA",ORIGIN="BICIMAD",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="USE_BIKE_UO",ORIGIN="BICIMAD",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="USE_BIKE_TOTAL_USE",ORIGIN="BICIMAD",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="USE_BIKE_UAAA",ORIGIN="BICIMAD",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="USE_BIKE_UAOA",ORIGIN="BICIMAD",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="USE_BIKE_UTADI",ORIGIN="BICIMAD",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="USE_BIKE_MBD",ORIGIN="BICIMAD",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="PAYMENT_UAAASTA",ORIGIN="BICIMAD",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="PAYMENT_UAAACTA",ORIGIN="BICIMAD",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="PAYMENT_",ORIGIN="BICIMAD",DESC.="")

DS_FIELDS <- add_row(DS_FIELDS,COLUMN="PAYMENT_UAAA_DI",ORIGIN="BICIMAD",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="AEMET_TM",ORIGIN="AEMET",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="AEMET_TMED",ORIGIN="AEMET",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="AEMET_PREC",ORIGIN="AEMET",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="AEMET_TMIN",ORIGIN="AEMET",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="AEMET_HORATMIN",ORIGIN="AEMET",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="AEMET_VELMEDIA",ORIGIN="AEMET",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="AEMET_RACHA",ORIGIN="AEMET",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="AEMET_TMAX",ORIGIN="AEMET",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="AEMET_TA",ORIGIN="",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="AEMET_PRESMAX",ORIGIN="AEMET",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="AEMET_RAINFALLS_SCALE",ORIGIN="AEMET",DESC.="")

DS_FIELDS <- add_row(DS_FIELDS,COLUMN="DAY",ORIGIN="",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="DAY_OF_WEEK",ORIGIN="",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="HOLIDAY",ORIGIN="",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="DAY_OF_WEEK_INT",ORIGIN="",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="DAY_OF_WEEK_EN",ORIGIN="",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="HOLIDAY_EN",ORIGIN="",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="MONDAY",ORIGIN="",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="TUESDAY",ORIGIN="",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="WEDNESDAY",ORIGIN="",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="THRUSDAY",ORIGIN="",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="FRIDAY",ORIGIN="",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="SATURDAY",ORIGIN="",DESC.="")
DS_FIELDS <- add_row(DS_FIELDS,COLUMN="SUNDAY",ORIGIN="",DESC.="")

knitr::kable(
  as.data.frame(DS_FIELDS),
  caption="Dataset Fields"
    
  )



  
```

## 2.2 Data Cleaning<a name="DataCleaning"></a>


3. EDA. Exploratory Data Analysis<a name="EDA"></a>
---------------------------------------------------

Vamos a trabajar con datos desde 2017 hasta 2019. Generaremos 3 datasets. Dividiremos el periodo 2017-2018 entre un conjunto de training (90%), y un conjunto de validación (10%) de forma aleatoria. Posteriormente utilizaremos un conjunto de 3 meses del año 2019 como conjunto de test.

```{r echo=FALSE, warning=FALSE, error=FALSE}
global_data_model <- tabla_all %>% select(YEAR,DAY,AEMET_TM_SCALE,USE_BIKE_TOTAL_USE,AEMET_RAINFALLS_SCALE,HOLIDAY,AEMET_TA_SCALE,MONDAY,TUESDAY, WEDNESDAY,THRUSDAY,FRIDAY,SATURDAY,SUNDAY,HOLIDAY,PAYMENT_UAAADI_SCALE,PAYMENT_UAAACTA_SCALE,PAYMENT_UAAASTA_SCALE,USE_BIKE_MBD_SCALE,AEMET_HORATMIN_SCALE,AEMET_VELMEDIA_SCALE,AEMET_RACHA_SCALE,AEMET_PRESMAX_SCALE)

data_model <- filter(global_data_model,   YEAR > 2016 & YEAR < 2019)
#USING CARET
trainModelIndex <- createDataPartition(data_model$USE_BIKE_TOTAL_USE, p = .9,list=FALSE,times=1)
trainModel <- data_model[ trainModelIndex,]
validationModel  <- data_model[-trainModelIndex,]
testModel <- filter(global_data_model, YEAR > 2018)

```

El siguiente gráfico representa los usos totales de bicicletas del servicio BICIMAD, englobando tanto con abonos anuales como con abonos temporales, en función del día para el período 2016-2019.

La línea azul representa el ajuste líneal de la serie y muestra un ligero crecimiento temporal
La línea roja es un ajuste GAM y muestra amplias fluctuaciones en el tiempo.



```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
# SUMMARY OF USE
#SUMMARY BY DAY POR DÍA
ggplot(data=tabla_all, aes(y=USE_BIKE_TOTAL_USE, x=tabla_all$DAY))  + geom_line() +  geom_smooth(se=FALSE, method="lm",colour="blue") + geom_smooth(se=FALSE,colour="red") + labs(y="Uso de bicicletas", x="Tiempo")


```
A continuación vamos a tratar de encontrar patrones del uso de la bicicleta, en relación con las variables disponibles.

El uso de la variable día de la semana se justifica por la posible existencia de distintos patrones entre días laborables (ir a trabajar, ir a clase) y los fines de semana (ócio, vida familiar, turismo).


```{r echo=FALSE, warning=FALSE}
  
#USOS POR DÍA DE LA SEMANA
#NumericDay = mean(DAY_OF_WEEK_INT),


USE_BY_WEEKDAY <- tabla_all %>% group_by(DAY_OF_WEEK_INT,DAY_OF_WEEK_EN) %>% summarise(Annual =mean(USE_BIKE_UA), Occasional= mean(USE_BIKE_UO), Mean=mean(USE_BIKE_TOTAL_USE), Median=median(USE_BIKE_TOTAL_USE))


ggplot(USE_BY_WEEKDAY, aes(y=Mean,x=reorder(DAY_OF_WEEK_EN,DAY_OF_WEEK_INT), fill=DAY_OF_WEEK_EN)) + geom_col() + labs(x="Día de la semana", y="Usos de la bicicleta",title="Usos totales por día de la semana")
  knitr::kable(
  as.data.frame(USE_BY_WEEKDAY),
  caption="Use of BICIMAD by day of Week"
    
  )

```

Se observa un mayor uso de la bicicleta los días laborables.

Podemos distinguir por el tipo de abono (anual, o temporal) para discernir si el uso es distinto.
```{r warning=FALSE,echo=FALSE , error=FALSE}
ggplot(USE_BY_WEEKDAY, aes(y=Occasional,x=reorder(DAY_OF_WEEK_EN,DAY_OF_WEEK_INT), fill=DAY_OF_WEEK_EN)) + geom_col() + labs(x="Día de la semana", y="Num Usos", title="Distribución de los usos ocasionales por día de la semana.")

```
```{r warning=FALSE,echo=FALSE , error=FALSE}
ggplot(USE_BY_WEEKDAY, aes(y=Annual,x=reorder(DAY_OF_WEEK_EN,DAY_OF_WEEK_INT), fill=DAY_OF_WEEK_EN)) + geom_col()  + labs(x="Día de la semana", y="Cantidad de usos", title="Distribución de los usos anuales por día de la semana.")

```
La distribución de los abonos ocasionales, muestra una estructura centrada en los fines de semana.

La distribución de los abonos anuales muestra el patrón inverso.


La variable HOLIDAY, procedente del dataset BICIMAD, permite agrupar los días festivos (fiestas locales, y nacionales, no laborables, así como sábados y domingos)

```{r warning=FALSE,echo=FALSE , error=FALSE, message=FALSE}

USOS_BY_HOLIDAY <- tabla_all %>% group_by(HOLIDAY_EN) %>% summarise(Mean=mean(USE_BIKE_TOTAL_USE), Median=median(USE_BIKE_TOTAL_USE),Sum=sum(USE_BIKE_TOTAL_USE))
ggplot(USOS_BY_HOLIDAY, aes(y=Mean,x=HOLIDAY_EN,fill=HOLIDAY_EN)) + geom_col()

knitr::kable(
  as.data.frame(USOS_BY_HOLIDAY),
  caption="Use of BICIMAD by Holiday"
    
  )
data_holiday <- select(tabla_all, HOLIDAY,USE_BIKE_TOTAL_USE)
holiday_cor <- cor(data_holiday)
knitr::kable(
  as.data.frame(holiday_cor),
  caption="Correlations by Holiday"
  )
```

El més no es considerado como una variable explicativa en si misma. Se va a utilizar aquí meramente para categorizar el uso durante el año.

```{r echo=FALSE,warning=FALSE, error=FALSE, message=FALSE}
# USOS POR MES


# tabla_all <- tabla_all %>% mutate(LABEL_MONTH = sapply(tabla_all$MONTH,LiteralMonth))

USOS_BY_MONTH <- tabla_all %>% group_by(LABEL_MONTH,MONTH) %>%  summarise(  mean = mean(TOTAL_USOS) , median = median(TOTAL_USOS) )
ggplot(USOS_BY_MONTH ,aes(y=mean, x=reorder(LABEL_MONTH,MONTH),fill=LABEL_MONTH)) + geom_col()

```

El patrón observado sugiere un mayor uso durante la primavera y el verano. Sin embargo baja en agosto. Esto tendría sentido, si tenemos en cuenta, como hemos visto antes, que la mayor parte del uso de la bicicleta se hace en días laborables, y, en España, Agosto es el més habitual de vacaciones.


Ahora vamos a ver el efecto de la temperatura en el uso de la bicicleta.

```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
#valores <- distinct(as.data.frame(tabla_all$TM))
USOS_BY_TM <- tabla_all %>% group_by(AEMET_TM) %>% summarise(mean=mean(TOTAL_USOS), median=median(TOTAL_USOS))
ggplot(USOS_BY_TM, aes(y=mean,x=AEMET_TM)) + geom_point() +geom_smooth() + ggrepel::geom_text_repel(aes(label=AEMET_TM), data=USOS_BY_TM)
```
El uso sube, con la temperatura, hasta un punto, a partir del cual, empieza a bajar.
Vamos a proceder a una transformación de la variable, tomando la distancia (en grados) con el valor con media más alta, usando la mediana de cada grupo como valor dentro de cada temperatura.

A continuación vamos a comprobar la correlación visualmente :

```{r echo=FALSE, warning=FALSE, error=FALSE}
USOS_BY_TA <- tabla_all %>% group_by(AEMET_TA) %>% summarise(cuantity = sum(TOTAL_USOS), mean=mean(TOTAL_USOS), median=median(TOTAL_USOS))
ggplot(USOS_BY_TA, aes(y=mean,x=AEMET_TA)) + geom_point() +geom_smooth(color="red",se=FALSE)  +geom_smooth(method="lm", color="blue", se=FALSE)  + ggrepel::geom_text_repel(aes(label=AEMET_TA), data=USOS_BY_TA)
```
La correlación obtenida es un poco mayor (aunque de signo inverso) a la temperatura media.Y tiene un sentido más coherente. El uso de la bicicleta cuando hay "una buene temperatura" (ni demasiado frío, ni demasiado calor).

A continuación vamos a analizar las precipitaciones

```{r echo=FALSE, warning=FALSE, error=FALSE}
USOS_BY_RAINFALL <- tabla_all %>% group_by(AEMET_RAINFALLS_INT) %>% summarise(count = n(),mean=mean(TOTAL_USOS), median=median(TOTAL_USOS),sum=sum(TOTAL_USOS))
ggplot(tabla_all, aes(y=TOTAL_USOS,x=AEMET_RAINFALLS_INT)) + geom_point()
```
Como era de esperar, el uso de la bicicleta se produce fundamentalmente, cuando no llueve, o lueve poco.




Otras variables climatológicas.

A continuación vamos a examinar otras variables meteorológicas de nuestro dataset como son
-La Temperatura mínima (TMIN)
-La temperatura máxima (TMAX)
-La hora de la temperatura media (HORATMIN)
-La velocidad del viento media (VELMEDIA)
-La racha
-La presión máxima (PRESMAX)


```{r echo=FALSE, warning=FALSE, error=FALSE}
view(tabla_all)
 # data_corrplot <-  select(tabla_all,TM_SCALE,USOS_TOTAL,RAINFALLS,HOLIDAY,TA,UAAADI,UAAACTA,UAAASTA)

  
  tabla_weather <- select(tabla_all,TOTAL_USOS,AEMET_RAINFALLS,AEMET_TA, AEMET_TM,AEMET_TMIN,AEMET_HORATMIN,AEMET_VELMEDIA,AEMET_RACHA ,AEMET_TMAX ,AEMET_PRESMAX)


  weather_cor <- cor(x=tabla_weather)
  #chart.Correlation(data_corrplot)
  #corrplot(M, method = "number")


  #chart.Correlation(weather_cor)
corrplot(weather_cor, method = "number")

```
Las variables de temperatura Mínima y Máxima están muy correlacionadas con la temperatura media (TM) y la temperatura estimada (TA), por lo cual serán excluidas de posteriores análisis

Finalmente, analizamos los usuarios activos, con abono anual, y lo comparamos con los usos totales
```{r echo=FALSE, warning=FALSE, error=FALSE}
USOS_BY_UAAADI <- tabla_all %>% group_by(PAYMENT_UAAADI) %>% summarise( cuantity = sum(TOTAL_USOS), mean=mean(TOTAL_USOS), median=median(TOTAL_USOS))
TEMP_TABLE <- select (tabla_all,PAYMENT_UAAADI,TOTAL_USOS)
cor(TEMP_TABLE)

ggplot(tabla_all, aes(y=PAYMENT_UAAADI,x=tabla_all$DAY)) + geom_point() + geom_point(aes(y=TOTAL_USOS,x=DAY)) +geom_smooth(aes(y=TOTAL_USOS,x=DAY))  

  
  
  ggrepel::geom_text_repel(aes(label=PAYMENT_UAAADI), data=USOS_BY_UAAADI)
```

4.  Models Construction<a name="Models"></a>
---------------------------------------------------



## 4.1 Selection Metodology<a name="SelectionMetodology"></a>

We are going to follow different metodologies to analyze the dataset. The main goal is to create a model able to explain the global use of bikes in BICIMAD.

This is a regression problem since we try to create models that explain or forecast one cuantitative label (in our case the variable USE_BIKE_TOTAL_USE)

In first place we will try one linear regression model. This is the one more easily to explain and unravel.

After that we will try other regression tastes like the ones based on the shrinkage   and non linear models.

Then , we will use some common ensemble methodologies (random forest and boosting)

Finally, we will compare the Accuracy of each model.

Its well known that, in general, the "machine learning" ensemble methods (such a random forest) are more powerful than traditional linear regression when forecast is the goal, but it is also true that the linear regression has many statistics to check the model. In other words, the ensemble methods we are going to use are more like a "black box"

As we said before, we are going to work with three subset of the dataset
-TrainModel
-ValidationModel
-TestModel

I have selected the data from the years 2017 and 2018, and i have divided (in a random way) into two, the TrainModel (90% of the data) and ValidationModel (10%)

Then i have create another subset with two months of 2019 for the TestModel.

I will get the RMSE of all of them and i will compare them at the end of the process.

The variables that will be finally included in the model will be:
-Total use of bikes per day (the label)
-Holiday (dummy)
-Days of week (from tuesday to sunday, 6 in total)
-Temperature adjusted
-Rainfalls
-Number of bikes availables
-Number of people with payment card active
AEMET_HORATMIN_SCALE
AEMET_VELMEDIA_SCALE
AEMET_RACHA_SCALE
AEMET_PRESMAX_SCALE


```{r  LINEAR_REGRESSION, echo=FALSE, warning=FALSE, error=FALSE }
```


## 4.2 Linear Regression<a name="LinearRegression"></a>
```{r LINEAR_REGRESSION, echo=FALSE, warning=FALSE, error=FALSE }


residplot <- function(fit, nbreaks=10) 
  { 
  z <- rstudent(fit)
  hist(z, breaks=nbreaks, freq=FALSE, 
  xlab="Studentized Residual", 
  main="Distribution of Errors")
  rug(jitter(z), col="brown") 
  curve(dnorm(x, mean=mean(z), sd=sd(z)),
  add=TRUE, col="blue", lwd=2) 
  lines(density(z)$x, density(z)$y,
  col="red", lwd=2, lty=2) 
  legend("topright",
  legend = c( "Normal Curve", "Kernel Density Curve"), 
  lty=1:2, 
  col=c("blue","red"), 
  cex=.7)
}
hat.plot <- function(fit) 
  {
p <- length(coefficients(fit))
n <- length(fitted(fit))
hatValues <-as.data.frame( hatvalues(fit))
hatValues <- hatValues  %>% mutate(NUM_ROW = row_number())
#trainModel$AEMET_TM_SCALE
hatLabels <- select(trainModel,USE_BIKE_TOTAL_USE, DAY,NUM_ROW,AEMET_TA_SCALE,HOLIDAY,AEMET_RAINFALLS_SCALE,AEMET_TM_SCALE)

hatAll <- merge(hatValues,hatLabels, by="NUM_ROW")
hatAll <- hatAll  %>% mutate(DATE_STRING = format(DAY,"%d/%b/%Y"))
hatHigh <- filter(hatAll, hatAll$`hatvalues(fit)` > 0.050)

ggplot() + geom_point(data=hatAll,aes(y=hatAll$`hatvalues(fit)`,x=DAY)) + ggrepel::geom_text_repel(data=hatHigh,aes(y=hatHigh$`hatvalues(fit)`,x=DAY,label=NUM_ROW))+geom_hline(yintercept=0.025, linetype="dashed", color = "red")

#ggplot() + geom_point(data=hatAll,aes(y=hatAll$`hatvalues(fit)`,x=AEMET_TA_SCALE)) + #ggrepel::geom_text_repel(data=hatHigh,aes(y=hatHigh$`hatvalues(fit)`,x=AEMET_TA_SCALE,label=NUM_ROW))+geom_hline(yint#ercept=0.025,linetype="dashed", color = "red")

#ggplot() + geom_point(data=hatAll,aes(y=hatAll$`hatvalues(fit)`,x=HOLIDAY)) + #ggrepel::geom_text_repel(data=hatHigh,aes(y=hatHigh$`hatvalues(fit)`,x=HOLIDAY,label=NUM_ROW))+geom_hline(yintercept=#0.025, linetype="dashed", color = "red")

#ggplot() + geom_point(data=hatAll,aes(y=hatAll$`hatvalues(fit)`,x=AEMET_RAINFALLS_SCALE)) + #ggrepel::geom_text_repel(data=hatHigh,aes(y=hatHigh$`hatvalues(fit)`,x=AEMET_RAINFALLS_SCALE,label=NUM_ROW))+geom_hli#ne(yintercept=0.025, linetype="dashed", color = "red")

#ggplot() + geom_point(data=hatAll,aes(y=hatAll$`hatvalues(fit)`,x=AEMET_TM_SCALE)) + #ggrepel::geom_text_repel(data=hatHigh,aes(y=hatHigh$`hatvalues(fit)`,x=AEMET_TM_SCALE,label=NUM_ROW))+geom_hline(yint#ercept=0.025, linetype="dashed", color = "red")

}

#FORMULA
EquationFormula <- USE_BIKE_TOTAL_USE ~ HOLIDAY +    AEMET_TA_SCALE +USE_BIKE_MBD_SCALE +TUESDAY+ WEDNESDAY+THRUSDAY+FRIDAY+SATURDAY+ SUNDAY + PAYMENT_UAAADI_SCALE +  AEMET_RAINFALLS_SCALE +AEMET_HORATMIN_SCALE + AEMET_VELMEDIA_SCALE+ AEMET_RACHA_SCALE+ AEMET_PRESMAX_SCALE 

#QUITAR DE AQUI
trainModel <- trainModel  %>% mutate(NUM_ROW = row_number())

# LINEAR REGRESSION
modelLineal <- lm(data = trainModel,formula=EquationFormula)
hat.plot(modelLineal)

#clenDs <- filter(trainModel, !trainModel$NUM_ROW %in% hatHigh$NUM_ROW)

#cleanmodelLineal <- lm(data = clenDs,formula=EquationFormula)
#summary(cleanmodelLineal)
#hat.plot(cleanmodelLineal)
#SUMMARY OF LM
#SUMMARYLINEAR <- summary(modelLineal)

# VALIDATION DATA
modelLinealFit <- predict(modelLineal,validationModel)
# TEST DATA
modelLinealFitForecast <- predict(modelLineal,testModel) 

#RMSE OF DATASETS
RMSETRAIN <- RMSE(trainModel$USE_BIKE_TOTAL_USE, modelLineal$fitted.values)
RMSELINEAL <- RMSE(validationModel$USE_BIKE_TOTAL_USE, modelLinealFit)
RMSELINEALForecast <- RMSE(testModel$USE_BIKE_TOTAL_USE, modelLinealFitForecast)

#RMSETRAIN
#RMSELINEAL
#RMSELINEALForecast
 
 corLinValidation <- cor(validationModel$USE_BIKE_TOTAL_USE, modelLinealFit)
 corLinForecast <-cor(testModel$USE_BIKE_TOTAL_USE, modelLinealFitForecast)
#corLinValidation
#corLinForecast

```


If we apply one linear regression to the selected variables, we get that all the variables has a p-value significative but three. The AEMET_HORATMIN_SCALE, AEMET_VELMEDIA_SCALE and AEMET_PRESMAX_SCALE. HOLIDAY, SATURDAY and PAYMENT_UAAADI_SCALE get the coefficients with higher values.  
```{r LINEAR_REGRESSION, echo=FALSE, warning=FALSE, error=FALSE }
SUMMARYLINEAR
```
We can see in the Q-Q plot that the residuals are not have a perfect normal distribution.
The residuals vs Fitted chart show that the model suffer heterocedasticity, because the variances are not homogeneous. This is related to the presence of values that has an extra relevance in the estimation (Residuals vs Leverage chart)
```{r LINEAR_REGRESSION, echo=FALSE, warning=FALSE, error=FALSE }
plot(modelLineal)
```
To check the heterocedasticity we will apply the Breusch-Pagan test, that confirm us the presence
```{r LINEAR_REGRESSION, echo=FALSE, warning=FALSE, error=FALSE }
library(lmtest)
resBP <- bptest(modelLineal)
resBP
cat("The p-value is:" ,  format(resBP$p.value, scientific = FALSE))
bptest(modelLineal)
 ```
```{r LINEAR_REGRESSION, echo=FALSE, warning=FALSE, error=FALSE }

 qqPlot(modelLineal,labels=row.names(AEMET_TA_SCALE) , simulate=TRUE, main="Q-Q Plot")
 residplot(modelLineal)
#crPlots(modelLineal) 
#ncvTest(modelLineal)

```
```{r LINEAR_REGRESSION, echo=FALSE, warning=FALSE, error=FALSE }


 #qqPlot(modelLineal,labels=row.names(AEMET_TA_SCALE) , simulate=TRUE, main="Q-Q Plot")
 residplot(modelLineal)
#crPlots(modelLineal) 
#ncvTest(modelLineal)
#spreadLevelPlot(modelLineal)
#VALIDACION GLOBAL
#resgvmmodel <- gvlma(x=modelLineal)
#resgvmmodel
#vif(modelLineal)
#sqrt(vif(modelLineal)) > 2
#outlierTest(modelLineal)
hat.plot(modelLineal)
#cutoff <- 4/(nrow(trainModel)-length(modelLineal$coefficients)-2)
#plot(modelLineal, which=4, cook.levels=cutoff)
#abline(h=cutoff, lty=2, col="red")
#avPlots(modelLineal,ask=FALSE,id.method="noteworthy")

#resgvmmodel <- gvlma(x=modelLineal)
#summary(resgvmmodel)
```

```{r GAM_REGRESSION, echo=FALSE, warning=FALSE, error=FALSE}


##gvmodel.del <- deletion.gvlma(resgvmmodel) 
##summary(gvmodel.del)
#boxTidwell(EquationFormula, data=trainModel)


##summary(powerTransform(trainModel$USE_BIKE_TOTAL_USE))
## trainModel <- trainModel  %>% mutate(USE_BIKE_TOTAL_USE_BOX_COX = trainModel$USE_BIKE_TOTAL_USE ##^0.953)
##EquationFormulaBOXCOX <- USE_BIKE_TOTAL_USE_BOX_COX ~ HOLIDAY +    AEMET_TA_SCALE ##+USE_BIKE_MBD_SCALE +TUESDAY+ WEDNESDAY+THRUSDAY+FRIDAY+SATURDAY+ SUNDAY + PAYMENT_UAAACTA_SCALE ##+ AEMET_RAINFALLS_SCALE

## modelLinealBOXCOX <- lm(data = trainModel,formula=EquationFormulaBOXCOX)
##summary(modelLinealBOXCOX)
##plot(modelLinealBOXCOX)


ip <- influencePlot(modelLineal, id.method="identify", main="Influence Plot",
sub="Circle size is proportional to Cook's distance")

 modelLinealFit <- as.data.frame(modelLinealFit)
 modelLinealFit <- modelLinealFit  %>% mutate(NUM_ROW = row_number())
valModelPred <- select(validationModel,USE_BIKE_TOTAL_USE,DAY)
valModelPred <- valModelPred  %>% mutate(NUM_ROW = row_number())
  PredData <- merge(x=modelLinealFit, y=valModelPred, by.x="NUM_ROW", by.y="NUM_ROW")
  ggplot(data=PredData, aes(x=modelLinealFit, y=USE_BIKE_TOTAL_USE)) + geom_point()+ geom_smooth(method="lm")
  
 modelLinealFitForecast <- as.data.frame(modelLinealFitForecast)
 modelLinealFitForecast <- modelLinealFitForecast  %>% mutate(NUM_ROW = row_number())
valModelPredForecast <- select(testModel,USE_BIKE_TOTAL_USE,DAY)
valModelPredForecast <- valModelPredForecast  %>% mutate(NUM_ROW = row_number())
  PredDataForecast <- merge(x=modelLinealFitForecast, y=valModelPredForecast, by.x="NUM_ROW", by.y="NUM_ROW")
  #PREDICCIONES 2019
  
  ggplot(data=PredDataForecast, aes(x=modelLinealFitForecast, y=USE_BIKE_TOTAL_USE)) + geom_point()+ geom_smooth(method="lm")
  
  ggplot(data=PredDataForecast) + geom_line(color="red",aes(x=DAY, y=USE_BIKE_TOTAL_USE)) + geom_line(color="blue",aes(x=DAY, y=PredDataForecast$modelLinealFitForecast))
  
  
```

## 4.3 Non Linear Regression<a name="NonLinearRegression"></a>

```{r GAM_REGRESSION, echo=FALSE, warning=FALSE, error=FALSE}

  
   #GAM MODEL

#trainModel$PAYMENT_UAAADI_SCALE
 Equation_GAM <- USE_BIKE_TOTAL_USE ~ HOLIDAY + s(AEMET_RAINFALLS_SCALE)+   s(AEMET_TA_SCALE) + s(USE_BIKE_MBD_SCALE) +TUESDAY+ WEDNESDAY+THRUSDAY+FRIDAY+SATURDAY+ SUNDAY+ s(PAYMENT_UAAADI_SCALE) 
 Equation_GAM2 <- USE_BIKE_TOTAL_USE ~ HOLIDAY + s(AEMET_RAINFALLS_SCALE)+   s(AEMET_TA_SCALE)  +TUESDAY+ WEDNESDAY+THRUSDAY+FRIDAY+SATURDAY+ SUNDAY+ s(PAYMENT_UAAADI_SCALE) +s(AEMET_HORATMIN_SCALE) + s(AEMET_VELMEDIA_SCALE)+ s(AEMET_RACHA_SCALE)+ s(AEMET_PRESMAX_SCALE) 
 

 modelGam <- gam(Equation_GAM,data = trainModel, method = "REML" )
 summary(modelGam)

  modelGam2 <- gam(Equation_GAM2,data = trainModel, method = "REML" )
 summary(modelGam2)

# modelGamFit <- predict(modelGam,validationModel)
# modelGam$sp
# plot(modelGam, pages = 4)
# gam.check(modelGam)
# concurvity(modelGam)

  modelGamFit2 <- predict(modelGam2,validationModel)
  modelGamFitForcast <- predict(modelGam2,testModel)
  
 modelGam2$sp

  plot(modelGam2, pages = 4)
 gam.check(modelGam2)
 concurvity(modelGam2)
ggplot(data=testModel,aes(x=DAY,y=testModel$USE_BIKE_TOTAL_USE)) + geom_point()



  
 #RMSEGAM <- RMSE(validationModel$USE_BIKE_TOTAL_USE, modelGamFit)
 RMSEGAM <- RMSE(validationModel$USE_BIKE_TOTAL_USE, modelGamFit2)
 RMSEGAMForeCast <- RMSE(testModel$USE_BIKE_TOTAL_USE, modelGamFitForcast)

 RMSEGAM
  RMSEGAMForeCast
   corGAM <- cor(modelGamFit2,validationModel$USE_BIKE_TOTAL_USE)
   corGAMForeCast <- cor(modelGamFitForcast,testModel$USE_BIKE_TOTAL_USE)
   
corGAM
corGAMForeCast
#   ggplot(data=trainModel, aes(x=USE_BIKE_TOTAL_USE, y=USE_BIKE_MBD_SCALE)) + geom_point()+ #geom_smooth(method="lm")

```
## 4.4 Shrinkage Regression<a name="Shrinkage"></a>
```{r Shrinkage,echo=FALSE, warning=FALSE, error=FALSE}

 #RIDGE LASSO MODEL


 matrixDataTrainModel <- model.matrix(USE_BIKE_TOTAL_USE ~ HOLIDAY + AEMET_RAINFALLS_SCALE+   AEMET_TA_SCALE +USE_BIKE_MBD_SCALE +TUESDAY+ WEDNESDAY+THRUSDAY+FRIDAY+SATURDAY+ SUNDAY + PAYMENT_UAAADI_SCALE,trainModel)
 matrixDataTestModel <-model.matrix(USE_BIKE_TOTAL_USE ~ HOLIDAY + AEMET_RAINFALLS_SCALE+   AEMET_TA_SCALE +USE_BIKE_MBD_SCALE +TUESDAY+ WEDNESDAY+THRUSDAY+FRIDAY+SATURDAY+ SUNDAY + PAYMENT_UAAADI_SCALE,validationModel)
 y <- trainModel$USE_BIKE_TOTAL_USE
 ridge.mod <-  cv.glmnet(matrixDataTrainModel,y, alpha=RIDGEGLMNET,lambda=grid,family = "gaussian",standardize = FALSE)
 print(ridge.mod)
 bestLambda <- ridge.mod$lambda.min
 RIDGEDATAFIT <- predict(ridge.mod,matrixDataTestModel, s=bestLambda)
 RMSERIDGEFIT <- RMSE(RIDGEDATAFIT,validationModel$USE_BIKE_TOTAL_USE)
 RMSERIDGEFIT
 corRidge <-cor(RIDGEDATAFIT,validationModel$USE_BIKE_TOTAL_USE)
corRidge

 lasso.mod <-  cv.glmnet(matrixDataTrainModel,y, alpha=LASSOGLMNET,lambda=grid,family = "gaussian",standardize = FALSE)
 print(lasso.mod)
 bestLambda <- lasso.mod$lambda.min
 LASSODATAFIT <- predict(lasso.mod,matrixDataTestModel, s=bestLambda)
 RMSELASSOFIT <- RMSE(LASSODATAFIT,validationModel$USE_BIKE_TOTAL_USE)
 RMSELASSOFIT
 LASSODATA <- predict(lasso.mod,matrixDataTrainModel, s=bestLambda)
 RMSELASSO <- RMSE(LASSODATA,validationModel$USE_BIKE_TOTAL_USE)
 RMSELASSO
 corTrainLasso <-cor(LASSODATA,trainModel$USE_BIKE_TOTAL_USE)
 corLasso <- cor(LASSODATAFIT,validationModel$USE_BIKE_TOTAL_USE)
 corTrainLasso
corLasso

 elastic.mod <-  cv.glmnet(matrixDataTrainModel,y, alpha=ELASTICGLMNET,lambda=grid,family = "gaussian",standardize = FALSE)
 print(elastic.mod)
 bestLambda <- elastic.mod$lambda.min
 ELASTICDATAFIT <- predict(lasso.mod,matrixDataTestModel, s=bestLambda)
 RMSEELASTICFIT <- RMSE(ELASTICDATAFIT,validationModel$USE_BIKE_TOTAL_USE)
 RMSEELASTICFIT
 corElastic <- cor(ELASTICDATAFIT,validationModel$USE_BIKE_TOTAL_USE)
  corElastic
  
```
## 4.5 Ensemble Methods.Trees<a name="EnsembleMethods"></a>
```{r echo=FALSE, warning=FALSE, error=FALSE}

set.seed(1)

modelRandomForest <-  randomForest(formula=EquationFormula,data=trainModel,importance=TRUE,
                                   proximity=TRUE)
RANDOMFORESTDATAFIT <- predict(modelRandomForest,validationModel)
RMSERANDOMFOREST <- RMSE(RANDOMFORESTDATAFIT,validationModel$USE_BIKE_TOTAL_USE)
RMSERANDOMFOREST
corRandom <- cor(RANDOMFORESTDATAFIT,validationModel$USE_BIKE_TOTAL_USE)
corRandom
#modelRandomForest
importance(modelRandomForest)
varImpPlot(modelRandomForest)
```
```{r echo=FALSE, warning=FALSE, error=FALSE}
modelRandomGBM <-  gbm(formula=EquationFormula,data=trainModel,distribution="gaussian",n.trees=5000,interaction.depth=4)
GBMDATAFIT <- predict(modelRandomGBM,validationModel,n.trees=5000)
RMSEGBM <- RMSE(GBMDATAFIT,validationModel$USE_BIKE_TOTAL_USE)
RMSEGBM
corGBM <- cor(GBMDATAFIT,validationModel$USE_BIKE_TOTAL_USE)
corGBM




summary(modelRandomGBM)

 GBMDATAFIT <- as.data.frame(GBMDATAFIT)
 GBMDATAFIT <- GBMDATAFIT  %>% mutate(NUM_ROW = row_number())
  PredData <- merge(x=GBMDATAFIT, y=valModelPred, by.x="NUM_ROW", by.y="NUM_ROW")
    PredData <- PredData  %>% mutate(DATE_STRING = format(DAY,"%d/%b/%Y"))
ggplot(data=PredData, aes(x=GBMDATAFIT, y=USE_BIKE_TOTAL_USE)) + geom_point() +  geom_smooth(method="lm")  #+geom_label(aes(label=DATE_STRING) )

                                                                                                                                     


```

```{r echo=FALSE, warning=FALSE, error=FALSE}

RMSELINEAL
RMSEGAM

RMSERIDGEFIT
RMSELASSOFIT
RMSEELASTICFIT
RMSERANDOMFOREST
RMSEGBM

corLin
corGAM
corRidge
corLasso
corElastic
corRandom
corGBM

```

5. Conclussions<a name="Conclussions"></a>
---------------------------------------------------

6. Future Investigations<a name="FutureInvestigations"></a>
---------------------------------------------------

7. Bibliography and Web References<a name="Bibliography"></a>
---------------------------------------------------

